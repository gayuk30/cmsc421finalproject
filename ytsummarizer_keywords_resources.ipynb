{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuvRsDENUdhV",
        "outputId": "f54a2eb0-5299-413d-9ee5-32af006e57b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.2.2)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries\n",
        "%pip install youtube-transcript-api\n",
        "%pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from getpass import getpass\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "# Securely input your OpenAI API key\n",
        "api_key = getpass(\"Enter your OpenAI API key: \")\n",
        "openai.api_key = api_key\n",
        "\n",
        "def get_video_id(url):\n",
        "    from urllib.parse import urlparse, parse_qs\n",
        "    query = urlparse(url).query\n",
        "    video_id = parse_qs(query).get('v')\n",
        "    return video_id[0] if video_id else None\n",
        "\n",
        "def fetch_transcript(video_id):\n",
        "    try:\n",
        "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        transcript_text = ' '.join([item['text'] for item in transcript_list])\n",
        "        return transcript_text\n",
        "    except Exception as e:\n",
        "        print(\"Failed to fetch transcript:\", str(e))\n",
        "        return None\n",
        "\n",
        "def summarize_text(text):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant tasked with providing a comprehensive summary that covers all key points in detail.\"},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            temperature=0.5,  # Slightly more creative\n",
        "            max_tokens=600    # Allow for a longer response\n",
        "        )\n",
        "        summary = response.choices[0].message['content']  # Correct attribute access for chat responses\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(\"Error during summarization:\", str(e))\n",
        "        return str(e)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = get_video_id(video_url)\n",
        "    if video_id:\n",
        "        print(\"Fetching transcript...\")\n",
        "        transcript = fetch_transcript(video_id)\n",
        "        if transcript:\n",
        "            print(\"Transcript fetched. Generating summary...\")\n",
        "            summary = summarize_text(transcript)\n",
        "            print(\"Summary:\", summary)\n",
        "        else:\n",
        "            print(\"Failed to fetch transcript.\")\n",
        "    else:\n",
        "        print(\"Invalid YouTube URL\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km8VmPP2UhOD",
        "outputId": "65511bb3-810f-4515-d002-f09fd4a22cb3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API key: ··········\n",
            "Enter the YouTube video URL: https://www.youtube.com/watch?v=I4pQbo5MQOs\n",
            "Fetching transcript...\n",
            "Transcript fetched. Generating summary...\n",
            "Summary: The sentence \"This statement is false\" creates a paradox as it refers to itself directly, leading to the question of its truth value. This thought experiment led Kurt Gödel to a groundbreaking discovery in the early 20th century regarding the limitations of mathematical proofs. Mathematical proofs are logical arguments based on axioms, which are fundamental statements about numbers. Gödel's work focused on the idea that some true mathematical statements are unprovable within a given set of axioms.\n",
            "\n",
            "Gödel developed a method to code mathematical statements into numbers, allowing mathematics to talk about itself. He constructed the first self-referential mathematical statement, \"This statement cannot be proved,\" which led to his Incompleteness Theorem. This theorem asserts that in any axiomatic system, there exist true statements that are unprovable. Gödel's work shattered the belief that all mathematical claims could eventually be proven or disproven, revealing the inherent limitations of mathematical systems.\n",
            "\n",
            "While Gödel's theorem initially caused uncertainty and debate in the mathematical community, it also inspired innovations in computer science and continues to influence research on provably unprovable statements. Mathematicians now acknowledge the presence of unprovable truths in mathematics, embracing the unknown and recognizing the complexity at the core of the quest for truth. Gödel's work fundamentally changed the way mathematicians approach mathematical reasoning and the nature of truth in mathematics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define stop words\n",
        "stop_words = list(set(stopwords.words('english')))\n",
        "\n",
        "# Create a TF-IDF vectorizer instance with stop words\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "\n",
        "# Fit the vectorizer to the summary and transform the text into a TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([summary])\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a dictionary to store word-frequency pairs\n",
        "word_frequencies = {}\n",
        "\n",
        "# Loop through each word and its TF-IDF score\n",
        "for col in tfidf_matrix.nonzero()[1]:\n",
        "    word = feature_names[col]\n",
        "    word_frequencies[word] = tfidf_matrix[0, col]\n",
        "\n",
        "# Sort the dictionary by TF-IDF score in descending order\n",
        "sorted_word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top N words and their TF-IDF scores\n",
        "top_n = 5\n",
        "print(\"Top\", top_n, \"keywords:\")\n",
        "for word, score in sorted_word_frequencies[:top_n]:\n",
        "    print(word, \"-\", score)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6yPaxc-XXl4",
        "outputId": "0489e42d-9da3-4541-ae2e-81a857844050"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 keywords:\n",
            "mathematical - 0.40993876680684926\n",
            "gödel - 0.3643900149394215\n",
            "statements - 0.31884126307199384\n",
            "mathematics - 0.22774375933713847\n",
            "true - 0.22774375933713847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3dipzGWty6o",
        "outputId": "791bd0f1-66f7-4187-a6e4-3c009e6be12f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import nltk\n",
        "import requests\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "\n",
        "def find_resources(word):\n",
        "    try:\n",
        "        search_results = search(word, num=1, stop=1, pause=2.0)\n",
        "        first_result_url = next(search_results)\n",
        "        print(f\"URL of the first search result for '{word}':\")\n",
        "        print(first_result_url)\n",
        "    except StopIteration:\n",
        "        print(\"No search results found.\")\n",
        "\n",
        "# Sample text\n",
        "text = summary\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Filter out stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "# Calculate frequency distribution\n",
        "fdist = FreqDist(filtered_words)\n",
        "\n",
        "\n",
        "# Print the most common words (keywords)\n",
        "print(\"Keywords:\")\n",
        "for word, frequency in fdist.most_common(5):\n",
        "    print(word + \":\", frequency)\n",
        "    find_resources(word)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUCppL7iVAvj",
        "outputId": "9991d8fc-59e1-4928-87cd-72e985117434"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords:\n",
            "mathematical: 9\n",
            "URL of the first search result for 'mathematical':\n",
            "https://www.merriam-webster.com/dictionary/mathematical\n",
            "gödel: 8\n",
            "URL of the first search result for 'gödel':\n",
            "https://en.wikipedia.org/wiki/Kurt_G%C3%B6del\n",
            "statements: 7\n",
            "URL of the first search result for 'statements':\n",
            "https://www.statementstile.com/\n",
            "true: 5\n",
            "URL of the first search result for 'true':\n",
            "https://www.merriam-webster.com/dictionary/true\n",
            "mathematics: 5\n",
            "URL of the first search result for 'mathematics':\n",
            "https://en.wikipedia.org/wiki/Mathematics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import requests\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_search_results(word):\n",
        "    try:\n",
        "        search_results = search(word, num=5, stop=5, pause=2.0)\n",
        "        return list(search_results)\n",
        "    except StopIteration:\n",
        "        return []\n",
        "\n",
        "def get_page_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup.get_text()\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "def extract_keywords(text):\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    fdist = FreqDist(filtered_words)\n",
        "    return [word for word, _ in fdist.most_common(5)]\n",
        "\n",
        "def classify_relevance(search_results, keywords):\n",
        "    relevance = []\n",
        "    for result in search_results:\n",
        "        title = result.split(' - ')[0].lower()\n",
        "        is_relevant = any(keyword in title for keyword in keywords)\n",
        "        relevance.append(1 if is_relevant else 0)\n",
        "    return relevance\n",
        "\n",
        "def train_classifier(X, y):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train = vectorizer.fit_transform(X)\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_train, y)\n",
        "    return clf, vectorizer\n",
        "\n",
        "def find_resources(text):\n",
        "    keywords = extract_keywords(text)\n",
        "    print(keywords)\n",
        "    search_results = get_search_results(' '.join(keywords))\n",
        "    relevance_labels = classify_relevance(search_results, keywords)\n",
        "    X_train, _, y_train, _ = train_test_split(search_results, relevance_labels, test_size=0.2, random_state=42)\n",
        "    clf, vectorizer = train_classifier(X_train, y_train)\n",
        "    X_test = vectorizer.transform(search_results)\n",
        "    predicted_labels = clf.predict(X_test)\n",
        "    relevant_results = [result for result, label in zip(search_results, predicted_labels) if label == 1]\n",
        "    return relevant_results\n",
        "\n",
        "# Find relevant resources\n",
        "relevant_resources = find_resources(summary)\n",
        "print(\"Relevant resources:\")\n",
        "for resource in relevant_resources:\n",
        "    print(resource)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W56m8vhVuiDZ",
        "outputId": "8a7d628a-1b58-48f0-9bf4-6a425b7aeec3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mathematical', 'gödel', 'statements', 'unprovable', 'statement']\n",
            "Relevant resources:\n",
            "https://www.reddit.com/r/math/comments/ugzwg1/unprovable_true_statements/\n",
            "https://www.reddit.com/r/compsci/comments/upi7g7/is_there_a_way_to_prove_that_a_specific_statement/\n",
            "https://www.reddit.com/r/math/comments/bx4lkh/do_we_have_any_idea_how_many_true_but_unprovable/\n"
          ]
        }
      ]
    }
  ]
}