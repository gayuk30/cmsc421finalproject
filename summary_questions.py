# -*- coding: utf-8 -*-
"""Copy of questions! copy of updated_yt_summary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/162HEOwW9r8xJOY0PzQUjgQ-v0R7gollJ
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install youtube-transcript-api
!pip install transformers
!pip install openai==0.28

import openai
from getpass import getpass
from youtube_transcript_api import YouTubeTranscriptApi

api_key = getpass("Enter your OpenAI API key: ")
openai.api_key = api_key

import spacy

nlp = spacy.load("en_core_web_sm")  # Load a SpaCy model for English

def split_into_sentences(text):
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents]

def fetch_transcript(video_id):
    try:
        # Assuming you are fetching a list of dictionaries with 'text' keys
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
        # Combine all text entries into a single string
        transcript = ' '.join([entry['text'] for entry in transcript_list])
        return transcript
    except Exception as e:
        print(f"Error obtaining transcript: {e}")
        return None

def get_youtube_transcript(video_id):
    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
        return transcript_list
    except Exception as e:
        print(f"Error obtaining transcript: {e}")
        return None

def convert_hms_to_seconds(hms):
    try:
        h, m, s = map(int, hms.split(':'))
        return h * 3600 + m * 60 + s
    except ValueError:
        raise ValueError("Invalid time format. Use hh:mm:ss.")

def extract_transcript_segment(transcript_list, start_time, end_time):
    segment = [item['text'] for item in transcript_list if start_time <= item['start'] < end_time]
    return ' '.join(segment)

def summarize_text(text):
    try:
        response = openai.Completion.create(
            engine="gpt-3.5-turbo-instruct",
            prompt=f"Summarize the following transcript:\n{text}",
            max_tokens=150
        )
        summary = response.choices[0].text.strip()
        return summary
    except Exception as e:
        print(f"Error generating summary: {e}")
        return None

def summarize_youtube_video(video_url):
    if "youtu.be" in video_url:
        video_id = video_url.split('/')[-1]
    elif "youtube.com" in video_url:
        video_id = video_url.split('v=')[1].split('&')[0]

    transcript_list = get_youtube_transcript(video_id)
    if not transcript_list:
        return "No transcript available for this video."

    use_timestamps = input("Would you like to specify start and end timestamps (yes/no)? ").strip().lower()
    if use_timestamps in ['yes', 'y']:
        try:
            start_time_hms = input("Enter the start time (hh:mm:ss): ").strip()
            end_time_hms = input("Enter the end time (hh:mm:ss): ").strip()
            start_time = convert_hms_to_seconds(start_time_hms)
            end_time = convert_hms_to_seconds(end_time_hms)
            transcript = extract_transcript_segment(transcript_list, start_time, end_time)
        except ValueError as e:
            return str(e)
    else:
        transcript = ' '.join([text['text'] for text in transcript_list])

    summary = summarize_text(transcript)
    return summary

def get_video_id(url):
    from urllib.parse import urlparse, parse_qs
    query = urlparse(url).query
    video_id = parse_qs(query).get('v')
    return video_id[0] if video_id else None

from transformers import pipeline

import json

# Load the T5 model for question generation
question_generator = pipeline("text2text-generation", model="valhalla/t5-small-qa-qg-hl")

def generate_questions(transcript):
    doc = nlp(transcript)  # Ensure this is a string
    questions = []
    for sent in doc.sents:
        prompt = f"generate question: {sent.text.strip()}"
        result = question_generator(prompt, max_length=50)
        if result:
            question = result[0]['generated_text'].strip()
            if question.endswith('?'):
                questions.append(question)
    return questions

def filter_sentences(sentences):
    filtered_sentences = []
    for sentence in sentences:
        if len(sentence) > 20 and "example non-informative phrase" not in sentence:
            doc = nlp(sentence)
            has_verb = any(token.pos_ == 'VERB' for token in doc)
            has_noun = any(token.pos_ == 'NOUN' for token in doc)
            if has_verb and has_noun:  # Check if there's at least one verb and one noun
                filtered_sentences.append(sentence)
    return filtered_sentences

def is_redundant(question):
    words = question.lower().strip('?').split()
    # Checking for repetition in key parts of the question
    if len(words) > 4:  # Consider checking repetition if the question has more than 4 words
        half = len(words) // 2
        first_half = words[:half]
        second_half = words[half:]
        if any(word in second_half for word in first_half):
            return True
    return False

def has_sufficient_information(question):
    # Ensure the question has at least 5 unique words, but adjust criteria based on observations
    unique_words = set(question.lower().split())
    # Exclude common question words from the count to focus on content words
    content_words = [word for word in unique_words if word not in ["what", "is", "the", "a", "of"]]
    return len(content_words) > 4

def is_logically_coherent(question):
    # Simplify to check for repetitive phrases
    phrases_to_check = ["what is the", "how does", "where is", "when is", "why does"]
    for phrase in phrases_to_check:
        # Count occurrences of each phrase
        if question.lower().count(phrase) > 1:
            return False
    return True


def filter_questions(questions):
    filtered = []
    for question in questions:
        if (len(question) > 10 and
            question.count('?') == 1 and
            not is_redundant(question) and
            has_sufficient_information(question) and
            is_logically_coherent(question)):
            filtered.append(question)
    return filtered

def video_id_from_url(video_url):
    if "youtu.be" in video_url:
        return video_url.split('/')[-1]
    elif "youtube.com" in video_url:
        return video_url.split('v=')[1].split('&')[0]
    return None

def save_questions_to_json(questions, filename='questions.json'):
    with open(filename, 'w') as file:
        json.dump(questions, file, indent=4)

if __name__ == "__main__":
    video_url = input("Enter the YouTube video URL: ")
    summary = summarize_youtube_video(video_url)
    if summary:
        print("Video summary:", summary)
    else:
        print("Failed to generate summary.")
    video_id = video_id_from_url(video_url)
    transcript = fetch_transcript(video_id)
    if transcript:
        questions = generate_questions(transcript)
        filtered_questions = filter_questions(questions)
        save_questions_to_json(filtered_questions)  # Save filtered questions to JSON
        print("Filtered questions saved to JSON.")
        for question in filtered_questions:
            print(question)
    else:
        print("Failed to retrieve or process transcript.")

